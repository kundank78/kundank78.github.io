"use strict";(self.webpackChunkkundan_dev=self.webpackChunkkundan_dev||[]).push([[477],{10:function(e){e.exports=JSON.parse('{"blogPosts":[{"id":"cheat-sheet-ckad","metadata":{"permalink":"/blog/cheat-sheet-ckad","source":"@site/blog/2023-03-16-ckad-notes.md","title":"Cheat Sheet for CKAD","description":"Cracking CKAD examination requires not only a solid comprehension of Kubernetes concepts but also proficiency in executing K8 commands. In this following, I have shared notes which I took during my CKAD preparation. I faced difficulty memorizing all Kubernetes object YAMLs and imperative commands, I consolidate all configurations in a single location as a comprehensive revision guide before the exam. I hope these notes can help you out too!","date":"2023-03-16T00:00:00.000Z","formattedDate":"March 16, 2023","tags":[{"label":"ckad","permalink":"/blog/tags/ckad"},{"label":"kubernetes","permalink":"/blog/tags/kubernetes"}],"readingTime":9.97,"hasTruncateMarker":true,"authors":[{"name":"Kundan Kumar","title":"Software Engineer at JP Morgan","url":"https://github.com/kundank78","imageURL":"https://avatars.githubusercontent.com/u/25195457"}],"frontMatter":{"slug":"cheat-sheet-ckad","title":"Cheat Sheet for CKAD","author":"Kundan Kumar","author_url":"https://github.com/kundank78","author_title":"Software Engineer at JP Morgan","author_image_url":"https://avatars.githubusercontent.com/u/25195457","tags":["ckad","kubernetes"]},"nextItem":{"title":"GitHub Action for Docusaurus","permalink":"/blog/docusaurus-gh-action"}},"content":"Cracking CKAD examination requires not only a solid comprehension of Kubernetes concepts but also proficiency in executing K8 commands. In this following, I have shared notes which I took during my CKAD preparation. I faced difficulty memorizing all Kubernetes object YAMLs and imperative commands, I consolidate all configurations in a single location as a comprehensive revision guide before the exam. I hope these notes can help you out too!\\n\\n\x3c!--truncate--\x3e\\n\\n### Setting Alias ..... A Must\\n```\\nalias k=kubectl\\n\\nexport do=\\"--dry-run=client -o yaml\\" \\nk create deploy nginx --image=nginx $do\\n\\nexport now=\\"--force --grace-period 0\\"\\nk delete pod pod_name $now\\n\\n```\\n\\n### K8s Cluster\\n- `k cluster-info`\\n- `k get nodes`\\n\\n### ReplicaSet\\n- `k get rs`\\n- `k scale --replicas=new_number rs replica_set_name`\\n\\n```yml\\n---\\napiVersion: apps/v1\\nkind: ReplicaSet\\nmetadata:\\n  name: replica_set_name\\n  labels:\\n    key: value\\nspec:\\n  replicas: no_of_pods\\n  selector:  #used to identify existing pods in env\\n    matchLabels:\\n      key: value\\n  template:\\n    metadata:\\n      name: pod_name\\n      labels:\\n        key: value\\n    spec:\\n      containers:\\n        - name: container_name\\n          image: image_name\\n```\\n\\n### Deployment\\n- `k create deploy deployment_name --image=image_name --replicas=no_of_pods`\\n- `k scale deploy deploy_name --replicas=no_of_pods`\\n- `k edit deploy deploy_name`                                              ----\x3e edit any field of deployment\\n- `k set image deploy deploy_name container_name=nginx:1.9.1 --record`     ----\x3e changed image to different version with record flag capturing cmd used\\n\\n- `k rollout status deploy deploy_name`\\n- `k rollout history deploy deploy_name`                                   ----\x3e show revisions of deployment\\n- `k rollout undo deploy deploy_name`                                      ----\x3e undo deployment to last revision\\n- `k rollout history deploy deploy_name --revision=number`                 ----\x3e describe deployment of revision number\\n- `k rollout undo deploy deploy_name --to-revision=number`                 ----\x3e rollback deployment to specific version\\n\\n```yml\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: deployment_name\\n  labels:\\n    key: value\\nspec:\\n  replicas: no_of_pods\\n  strategy:\\n    rollingUpdate:\\n      maxSurge: 25%\\n      maxUnavailable: 25%\\n    type: RollingUpdate\\n  selectors:\\n    matchLabels:\\n      key: value\\n  template:\\n    metadata:\\n      name: pod_name\\n      labels:\\n        key: value\\n    spec:\\n      containers:\\n        - name: container_name\\n          image: image_name\\n          ports:\\n            - containerPort: 8080\\n```\\n\\n### Namespace\\n- `k config set-context --current --namespace=namespace_name`\\n- `k port-forward svc/my-service 5000`\\n\\n```yml\\n---\\napiVersion: v1\\nkind: ResourceQuota\\nmetadata:\\n  name: compute-quota\\n  namespace: dev\\nspec:\\n  hard:\\n    pods: \\"10\\"\\n    requests:\\n      cpu: \\"4\\"\\n      memory: \\"5Gi\\"\\n    limits:\\n      cpu: \\"10\\"\\n      memory: 10Gi\\n```\\n\\n### Pods Configuration\\n\\n##### Multi container pod: share same lifecycle & network\\n- Patterns: Side Car, Ambassador, Adapter\\n\\n- POD Conditions\\n- PodScheduled -> Initialized -> ContainersReady -> Ready\\n\\n- `k logs -f pod_name container_name`               # tail logs for container_name for multi container pod\\n- `k replace -f pod.yaml --force`                   # replace existing pod with new one\\n- `k get pods -l key=value --no-headers | wc -l`    # count of pod\\n- `k get pod --show-labels`\\n- `k label po -l \\"app in(v1,v2)\\" tier=web`\\n\\n##### Taint Node\\n- `k taint node node_name key=value:taint-effect`  | Effects: NoSchedule, PreferNoSchedule, NoExecute\\n- `k taint node node_name key=value:taint-effect-` | Remove taint\\n- `k label node node_name key=value`\\n\\n```yml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod_name\\n  labels:\\n    key: value\\n  annotations:\\n    buildVersion: 1.34\\n  namespace: ns_name\\nspec:\\n  restartPolicy: Always                   # Always by default, Never & OnFailure\\n  serviceAccountName: service_acc_nm      # cannot be edited in pod but in deployment this can be edited and deployment will handle rollout for us\\n  automountServiceAccountToken: false     # doesn\'t mount default service account\\n  securityContext:                        # pod level security context, can be moved to container level\\n    runAsUser: 1000\\n  volumes:\\n    - name: pvc_volume\\n      persistentVolumeClaim:\\n        claimName: pvc_name\\n    - name: config_map_volume\\n      configMap:\\n        name: config_map_name\\n    - name: secret_volume                 # creates file for each key & value as secret\\n      secret:\\n        secretName: secret_name\\n    - name: empty_dir_vol                 # exists as long as pod on node disk, ram or network storage\\n      emptyDir:\\n        sizeLimit: 500Mi\\n    - name: host_volume\\n      hostPath:                           # mount file or directory from host node\'s filesystem\\n        type: Directory | DirectoryOrCreate\\n        path: /data\\n  affinity:\\n    nodeAffinity:\\n      requiredDuringSchedulingIgnoredDuringExecution:    # preferredDuringSchedulingIgnoredDuringExecution\\n        nodeSelectorTerms:\\n          - matchExpressions:\\n              - key: size\\n                operator: NotIn                          # In, NotIn, Exists, DoesNotExist, Gt and Lt.\\n                values:\\n                  - Small\\n  initContainers:\\n    - name: install\\n      image: image_name\\n  containers:\\n    - name: container_name\\n      image: image_name\\n      ports:\\n        - containerPort: port to expose\\n      command: [\\"sleep2.0\\"]               # overrides the entrypoint in docker\\n      args: [\\"10\\"]                        # adds as param in docker run cmd\\n      resources:                          # Scheduled on node if sum of requests of all container is less than node limit\\n        requests:\\n          cpu: \\"1\\"                        # \\"1\\" -> 1000m | 1m -> 1v cpu aws | If request not specified, it matches limit (same for memory)\\n          memory: \\"512Mi\\"\\n        limits:                           # container cannot exceeds its cpu limit    \\n          cpu: \\"2\\"                        # container can use more memory than limit but if it is continuous it will terminate | If limit not specified, \\n          memory: \\"1Gi\\"                     it can use all of node\'s memory and finally get killed | If limit range is defined for namespace it uses it as default\\n      securityContext:\\n        capabilities:\\n          add: [ \\"MAC_ADMIN\\" ]\\n      volumeMounts:\\n        - mountPath: /opt                 # this will be in sync\\n          name: volume_name\\n      env:                                # adding env variables\\n        - name: APP_COLOR\\n          value: pink\\n        - name: APP_COLOR\\n          valueFrom:\\n            configMapKeyRef:\\n              name: config_map_name\\n              key: KEY1\\n        - name: APP_COLOR\\n          valueFrom:\\n            secretKeyRef:\\n              name: secret_name\\n              key: KEY1\\n      envFrom:\\n        - configMapRef:\\n            name: config_map_name\\n        - secretRef:\\n            name: secret_name\\n      tolerations:\\n        - key: \\"app\\"\\n          operator: \\"Equal\\"\\n          value: \\"blue\\"\\n          effect: \\"NoSchedule\\"\\n      nodeSelector:                         # label node with same\\n        key: value\\n      readinessProbe:\\n        periodSeconds: 5\\n        initialDelaySeconds: 15\\n        failureThreshold: 8\\n        httpGet:\\n          path: /api/ready\\n          port: 5000\\n        tcpSocket:\\n          port: 8080\\n        exec:\\n          command:\\n            - cat\\n            - /app/is_ready\\n      livenessProbe:\\n        periodSeconds: 5\\n        initialDelaySeconds: 15\\n        failureThreshold: 8\\n        httpGet:\\n          path: /api/ready\\n          port: 5000\\n        tcpSocket:\\n          port: 8080\\n        exec:\\n          command:\\n            - cat\\n            - /app/is_ready\\n```\\n\\n### Config Map\\n\\n- `k create configmap config_map_name --from-literal=KEY1=VALUE1 --from-literal=KEY2=VALUE2`\\n- `k create configmap config_map_name --from-file=file_name.properties`\\n\\n```yml\\n---\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: config_map_name\\nspec:\\n  key1: value1\\n  key2: value2\\n```\\n\\n### Secrets\\n- `echo -n \'encode\' | base64`\\n- `echo -n \'decode\' | base64 --decode`\\n- `k create secret secret_name --from-literal=KEY1=VALUE1 --from-literal=KEY2=VALUE2`\\n- `k create secret secret_name --from-file=file_name.properties`\\n\\n```yml\\n---\\napiVersion: v1\\nkind: Secret\\nmetadata:\\n  name: secret_name\\nspec:\\n  key1: value1\\n  key2: value2\\n```\\n### Service Accounts\\n```\\nwhen service account is created a token is generated as secret | This used happen before kube 1.24, after\\n1.24 you can create token using `k create token service_account_name` this prints token with expiry time\\nToken can be passed as a bearer token while calling kube apis\\nEach namespace has its own service account\\nPod when created is mounted with volume having token created via token request api(has expiry)\\n```\\n``` \\n k create sa service_account_name\\n k create token <sa_name>\\n``` \\nManually create long-lived token for service account\\nAnnotate secret with kubernetes.io/service-account.name: <sa_name> and controller will auto-inject token inside the secret\\n\\n\\n\\n### Jobs & CronJob\\n- `k create job job_name --image=image_name -- command`\\n- `k create cronjob cron_name --image=image_name --schedule=\\"* * * * *\\"`\\n\\n```yml\\n---\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  name: job_name\\nspec:\\n  completions: 3                            ----\x3e no. of successful job completions | new pods are created until this number is reached\\n  parallelism: 3                            ----\x3e creates pods in parallel\\n  template:\\n    spec:\\n      containers:\\n        - name: container_name\\n          image: image_name\\n          command:\\n            - cmd1\\n      restartPolicy: Never\\n\\n---\\napiVersion: batch/v1beta1\\nkind: CronJob\\nmetadata:\\n  name: cron-job-name\\nspec:\\n  schedule: \\"*/1 * * * *\\"\\n  jobTemplate:\\n    spec:\\n      completions: 3\\n      parallelism: 3\\n      template:\\n        spec:\\n          containers:\\n            - name: container_name\\n              image: image_name\\n          restartPolicy: Never\\n```\\n\\n\\n### Service\\n```yml\\n---\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: service_name\\nspec:\\n  type: NodePort\\n  ports:\\n    - targetPort: 80                                   # container\'s port\\n      port: 80                                         # service port\\n      nodePort: 30008\\n  selector:\\n      key: value\\n      key1: value1\\n```\\n\\n##### Load Balancing through services is done in random fashion\\n##### ClusterIP : creates a virtual ip\\n##### LoadBalancer\\n##### NodePort (Range: 30000 - 32767): Exposes node port for every node if pods are distributed\\n\\n- `k expose pod pod_name --port=port_number --name=svc_name`        ----\x3e create cluster ip service with pod\'s label as selectors\\n- `k create svc clusterip svc_name --tcp=?:?`                       ----\x3e create cluster ip service with selector\'s as app=svc_name\\n\\n- `k expose pod pod_name --port=80 --type=NodePort`                 ----\x3e create node port service with pod\'s label as selectors\\n- `k create svc nodeport svc_name --tcp=?:? --node-port=node_port`  ----\x3e create nodeport svc with defined node port but doesn\'t use pod\'s labels as selectors\\n\\n\\n### Ingress\\n\\n```yml\\n---\\napiVersion: networking.k8s.io/v1\\nkind: Ingress\\nmetadata:\\n  name: ingress_name\\nspec:\\n  rules:\\n    - host: host_name\\n      http:\\n        paths:\\n          - path: /path1\\n            pathType: Prefix\\n            backend:\\n              service:\\n                name: svc_name\\n                port:\\n                  number: port_number\\n          - path: /path2\\n            pathType: Prefix\\n            backend:\\n              service:\\n                name: svc_name2\\n                port:\\n                  number: port_number2\\n```\\n\\n- `kubectl create ingress <ingress-name> --rule=\\"host/path=service:port\\"`\\n- `kubectl create ingress ingress-test --rule=\\"wear.my-online-store.com/wear*=wear-service:80\\"`\\n\\n### Network Policy\\n\\n```yml\\n---\\napiVersion: networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata:\\n  name: name\\n  namespace: pod_namespace\\nspec:\\n  podSelector:\\n    matchLabels:\\n      key: value\\n  policyTypes:                           # If only policyTypes is present it blocks all Ingress & Egress traffic\\n    - Ingress\\n    - Egress\\n  ingress:\\n    - from:\\n        - podSelector:\\n            matchLabels:\\n              key: other-pod\\n          namespaceSelector:             # If only namespace selector defined all pods under given ns will be able to access\\n            matchLabels:\\n              name: other_ns_name\\n        - ipBlock:\\n            cidr: 192.168.5.10/32\\n      ports:\\n        - protocol: TCP\\n          port: 3306                     # incoming traffic on port\\n  egress:\\n    to:\\n      - ipBlock:\\n          cidr: 192.168.5.10/32\\n    ports:\\n      - protocol: TCP\\n        port: 80                         # port on server ip\\n```\\n\\n### Volumes\\n\\n```yml\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: pv_name\\nspec:\\n  accessModes:\\n    - ReadWriteOnce\\n    - ReadOnlyMany\\n    - ReadWriteMany\\n  capacity:\\n    storage: 1Gi\\n  persistentVolumeReclaimPolicy: Recycle | Retain | Delete\\n  awsElasticBlockStore:\\n    volumeID: volume_id\\n    fsType: ext4\\n  hostPath:\\n    path: \\"/mnt/data\\"\\n\\n---\\napiVersion: v1\\nkind: PersistentVolumeClaim\\nmetadata:\\n  name: pvc_name\\nspec:\\n  storageClassName: manual | normal\\n  accessModes:\\n    - ReadWriteOnce\\n  resources:\\n    requests:\\n      storage: 500Mi\\n\\n```\\n\\n### Kube API Server & API Groups\\n\\n##### Edit the kube-apiserver static pod configured by kubeadm to pass in the user details.\\n##### The file is located at /etc/kubernetes/manifests/kube-apiserver.yaml\\n\\n- `k config use-context context_name`\\n- `k config view`\\n- `k config set-context --current --namespace=name`\\n\\n\\n### API Groups\\n- `k proxy`     ---\x3e exposes kube api on local\\n- `k api-resources --namespaced=true `\\n- Actions- list, get, create, update, delete, watch\\n\\n```yml\\n---\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: kube-apiserver\\n  namespace: kube-system\\nspec:\\n  containers:\\n    - command:\\n        - kube-apiserver\\n        - --authorization-mode=Node,RBAC\\n          <content-hidden>\\n        - --basic-auth-file=/tmp/users/user-details.csv\\n      image: k8s.gcr.io/kube-apiserver-amd64:v1.11.3\\n      name: kube-apiserver\\n      volumeMounts:\\n        - mountPath: /tmp/users\\n          name: usr-details\\n          readOnly: true\\n  volumes:\\n    - hostPath:\\n        path: /tmp/users\\n        type: DirectoryOrCreate\\n      name: usr-details\\n```\\n\\n### Roles\\n- `k create role role_name --verb=get,ist --resources=pods,pods/status --resource-name`\\n- `k create rolebinding role_binding_name --clusterrole=cluster_role_name --user=user_name --namespace=ns_name`\\n- `k auth can-i <cmd> -as user`\\n\\n```yml\\nkind: Role\\napiVersion: rbac.authorization.k8s.io/v1\\nmetadata:\\n  namespace: default\\n  name: pod-reader\\nrules:\\n  - apiGroups: [\\"\\"] # \\"\\" indicates the core API group\\n    resources: [\\"pods\\", \\"pods/log\\"]\\n    verbs: [\\"get\\", \\"watch\\", \\"list\\"] | # [\\"*\\"] --\x3e everything\\n\\n---\\n# This role binding allows \\"user1\\" to read pods in the \\"default\\" namespace.\\nkind: RoleBinding\\napiVersion: rbac.authorization.k8s.io/v1\\nmetadata:\\n  name: read-pods\\n  namespace: default\\nsubjects:\\n  - kind: User\\n    name: user1 # Name is case-sensitive\\n    apiGroup: rbac.authorization.k8s.io\\nroleRef:\\n  kind: Role #this must be Role or ClusterRole\\n  name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to\\n  apiGroup: rbac.authorization.k8s.io\\n```\\n\\n\\n### API Versions\\n\\n##### vXalphaY --\x3e vXbetaY --\x3e vX\\n\\n##### Alpha -> Not enabled by default\\n##### Beta -> Enabled by default\\n##### GA(Stable) -> Enabled by default\\n\\n##### Preferred version -> version k8s will use while retrieving info\\n##### Storage Version -> version objects will converted to while storing in etcd cluster\\n\\n### API Deprecations\\n1. Api elements can be removed only by incrementing API version of group\\n2. Api objects must be able to round trip between API versions in a given release without information loss\\n   with exception of whole REST resources which don\'t exist in some version\\n3. Other than the most recent API version in each track, older API version must be supported after their announced\\n   deprecation for a duration of no less than-\\n   a. GA (stable)- 12 months or 3 releases (whichever is longer)\\n   b. Beta - 9 months or 3 releases (whichever is longer)\\n   c. Alpha - 0 releases\\n   In Kubernetes versions -> X.Y.Z\\n   Where X stands for major, Y stands for minor and Z stands for patch version.\\n- `k convert -f <old_file> --output-version group/version`\\n- Add `--runtime-config={api-group}/{version}`  --\x3e enable new version\\n\\n### Helm\\n\\n```\\nhelm repo add repository-name url\\nhelm repo remove repository-name\\nhelm repo update\\nhelm list\\nhelm search hub package_name\\nhelm search repo package_name\\nhelm show chart repo/package\\nhelm show values repo/package\\n\\nhelm get manifest release_name\\nhelm install release_name chart_name\\nhelm status release_name\\nhelm upgrade release_name repo/package\\nhelm history release_name\\nhelm rollback release_name version\\nhelm uninstall release-name\\nhelm pull --untar repo/chart_name\\n```"},{"id":"docusaurus-gh-action","metadata":{"permalink":"/blog/docusaurus-gh-action","source":"@site/blog/2021-01-17-docusaurus-gh-action.md","title":"GitHub Action for Docusaurus","description":"I got tired of deploying my Docusaurus website to GitHub Pages manually, and decided to do something about it using GitHub Action.","date":"2021-01-17T00:00:00.000Z","formattedDate":"January 17, 2021","tags":[{"label":"docusaurus","permalink":"/blog/tags/docusaurus"},{"label":"github-action","permalink":"/blog/tags/github-action"},{"label":"ci","permalink":"/blog/tags/ci"}],"readingTime":1.485,"hasTruncateMarker":true,"authors":[{"name":"Kundan Kumar","title":"Software Engineer at JP Morgan","url":"https://github.com/kundank78","imageURL":"https://avatars.githubusercontent.com/u/25195457"}],"frontMatter":{"slug":"docusaurus-gh-action","title":"GitHub Action for Docusaurus","author":"Kundan Kumar","author_url":"https://github.com/kundank78","author_title":"Software Engineer at JP Morgan","author_image_url":"https://avatars.githubusercontent.com/u/25195457","tags":["docusaurus","github-action","ci"]},"prevItem":{"title":"Cheat Sheet for CKAD","permalink":"/blog/cheat-sheet-ckad"}},"content":"I got tired of deploying my Docusaurus website to GitHub Pages manually, and decided to do something about it using GitHub Action.\\n\\nInitially, I was planning to follow the [official guide](https://v2.docusaurus.io/docs/deployment#triggering-deployment-with-github-actions) on doing so. However, it was actually much more complicated than I liked. I did not really want to generate and store a SSH key on GitHub. Too much effort man.\\n\\nI decided it was better off for me to write my own script. Here it is:\\n\\n\x3c!--truncate--\x3e\\n\\n## deploy-docusaurus.yml\\n\\n:::caution\\n\\nThe script below assumes that your Docusaurus website resides at `/website` of your repo. If that is not the case for you, you will need to:\\n\\n- Change `cd website` to `cd <docu_site_root>`, or delete the entire line if your Docusaurus website is at the root of your repo `/`\\n- Change `build_dir`\'s value from `website/build` to `<docu_site_root>/build`, or `build` if your Docusaurus website is at the root of your repo `/`\\n\\n:::\\n\\n```yml\\nname: deploy-docusaurus\\n\\non:\\n  push:\\n    branches: [main]\\n  pull_request:\\n    branches: [main]\\n\\n  # Allows you to run this workflow manually from the Actions tab\\n  workflow_dispatch:\\n\\n# A workflow run is made up of one or more jobs that can run sequentially or in parallel\\njobs:\\n  publish:\\n    runs-on: ubuntu-latest\\n    steps:\\n      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it\\n      - name: Check out repo\\n        uses: actions/checkout@v2\\n      # Node is required for npm\\n      - name: Set up Node\\n        uses: actions/setup-node@v2\\n        with:\\n          node-version: \\"12\\"\\n      # Install and build Docusaurus website\\n      - name: Build Docusaurus website\\n        run: |\\n          cd website\\n          npm install \\n          npm run build\\n      - name: Deploy to GitHub Pages\\n        if: success()\\n        uses: crazy-max/ghaction-github-pages@v2\\n        with:\\n          target_branch: gh-pages\\n          build_dir: website/build\\n        env:\\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\\n```\\n\\n:::note\\n\\nGitHub will automatically add `GITHUB_TOKEN` to Secrets. You need not do so. See [this](https://docs.github.com/en/actions/reference/authentication-in-a-workflow) for more information.\\n\\n:::"}]}')}}]);