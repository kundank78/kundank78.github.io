"use strict";(self.webpackChunkkundan_dev=self.webpackChunkkundan_dev||[]).push([[477],{10:function(e){e.exports=JSON.parse('{"blogPosts":[{"id":"unix-system-calls","metadata":{"permalink":"/blog/unix-system-calls","source":"@site/blog/2024-05-17-unix-system-calls.md","title":"System Calls","description":"System calls, as the name suggests, are requests made by a user program to the operating system to perform","date":"2024-05-17T00:00:00.000Z","formattedDate":"May 17, 2024","tags":[{"label":"unix","permalink":"/blog/tags/unix"},{"label":"system-calls","permalink":"/blog/tags/system-calls"}],"readingTime":2.3,"hasTruncateMarker":false,"authors":[{"name":"Kundan Kumar","title":"Software Engineer at JP Morgan","url":"https://github.com/kundank78","imageURL":"https://avatars.githubusercontent.com/u/25195457"}],"frontMatter":{"slug":"unix-system-calls","title":"System Calls","author":"Kundan Kumar","author_url":"https://github.com/kundank78","author_title":"Software Engineer at JP Morgan","author_image_url":"https://avatars.githubusercontent.com/u/25195457","tags":["unix","system-calls"]},"nextItem":{"title":"Here I am","permalink":"/blog/here-i-am"}},"content":"System calls, as the name suggests, are requests made by a user program to the operating system to perform \\ncertain tasks on behalf of the program. They resemble regular function calls initiated by the user program. \\nWhether it\'s writing to or reading from a file, making network calls, or other operations, system calls \\nwork behind the scenes to facilitate these actions. Today, we\'ll delve into the read and write system calls \\nand explore the intricacies surrounding them.\\n\\n### Read System Call\\n\\nGolang provides more abstracted `os` package to do file operation. But you can directly use `syscall` package\\nif needed.\\n\\n```go\\nfunc main() {\\n    fd, err := syscall.Open(\\"example.txt\\", syscall.O_RDONLY, 0)\\n    if err != nil {\\n        fmt.Println(\\"Error opening file:\\", err)\\n        return\\n    }\\n    defer syscall.Close(fd)\\n\\n    buffer := make([]byte, 1024) // buffer to store read data\\n    bytesRead, err := syscall.Read(fd, buffer)\\n    if err != nil {\\n        fmt.Println(\\"Error reading file:\\", err)\\n        return\\n    }\\n    fmt.Printf(\\"Read %d bytes from file:\\\\n%s\\", bytesRead, string(buffer[:bytesRead]))\\n}\\n```\\n\\nAbove code opens a file using `syscall.Open` call which returns a file descriptor. Kernel checks if requested data is present in \\nbuffer cache, if data is present it copies the data into user buffer. Otherwise, kernel makes a disk read to fetch the data and store it\\nin buffer. Common pattern is to read more than what is required so next time `read syscall` will result in cache hit.\\n\\n![read-sys.png](images/read-sys.png)\\n\\n\\n### Write System Call\\n\\n\\n\\n```go\\nfunc main() {\\n    fd, err := syscall.Open(\\"example.txt\\", syscall.O_WRONLY | syscall.O_CREAT | syscall.O_TRUNC, 0644)\\n    if err != nil {\\n        fmt.Println(\\"Error opening or creating file:\\", err)\\n        return\\n    }\\n    defer syscall.Close(fd)\\n    \\n    data := []byte(\\"Hello, World!\\\\n\\")\\n    bytesWritten, err := syscall.Write(fd, data)\\n    if err != nil {\\n        fmt.Println(\\"Error writing to file:\\", err)\\n        return\\n    }\\n    fmt.Printf(\\"Wrote %d bytes to file\\\\n\\", bytesWritten)\\n}\\n```\\n\\nHere `syscall.Write` writes the data in os write buffer & returns. It\'s the OS responsibility to write the data \\nto storage layer. `fsync syscall` can be used to synchronize file\'s data & metadata (permissions, timestamps & directory) \\nto storage medium, ensuring durability & system integrity.\\n\\n\\n![write-sys.png](images/write-sys.png)\\n\\n### File Description Offset\\n\\n- File Descriptor: a number to uniquely identify a open file. When file is opened, syscall returns\\n  a file descriptor not the file path. This file descriptor points to the file description (stores metadata).\\n\\nAll read/write in a file happens at an offset maintained by the file description which is referenced by file descriptor. \\nIf write bytes are more than the file size, it simply expands to accommodate the bytes to be written. \\n\\n![img_2.png](images/description-offset.png)\\n\\n![img_3.png](images/fd-description.png)\\n\\n- `truncate syscall`: shrinks or expands the file size based on the given input. Expanding the file will add zeroed out bytes\\n- `lseek syscall`: moves the marker to given byte offset. You can define offset greater than file size which will expand the file."},{"id":"here-i-am","metadata":{"permalink":"/blog/here-i-am","source":"@site/blog/2024-05-06-hello-world.md","title":"Here I am","description":"Fueled by my midnight coffee & desperate need to stop procrastinating.","date":"2024-05-06T00:00:00.000Z","formattedDate":"May 6, 2024","tags":[],"readingTime":0.625,"hasTruncateMarker":false,"authors":[{"name":"Kundan Kumar","title":"Software Engineer at JP Morgan","url":"https://github.com/kundank78","imageURL":"https://avatars.githubusercontent.com/u/25195457"}],"frontMatter":{"slug":"here-i-am","title":"Here I am","author":"Kundan Kumar","author_url":"https://github.com/kundank78","author_title":"Software Engineer at JP Morgan","author_image_url":"https://avatars.githubusercontent.com/u/25195457","tags":[]},"prevItem":{"title":"System Calls","permalink":"/blog/unix-system-calls"},"nextItem":{"title":"Cheat Sheet for CKAD","permalink":"/blog/cheat-sheet-ckad"}},"content":"Fueled by my midnight coffee & desperate need to stop procrastinating. \\nFinally starting my tech blog - spent a good solid 30 minutes finding a good blogging site and copying its vibe.\\n\\n#### So, here is my `Hello World!!` moment \\n\\nI am Kundan Kumar, a software engineer since 2020. JAVA is my coding jam for work. \\nLately, obsessed with Distributed Systems & Database Internals and also trying to convince myself Golang is my best friend.\\nMy blog might not be the best ones you will find in the chaotic internet sea, but I will try to give some good tech tidbits \\n& inspire you a bit. \\nMy goal is to learn, share & maybe, just maybe, make this blog worth your time. Wish me luck!."},{"id":"cheat-sheet-ckad","metadata":{"permalink":"/blog/cheat-sheet-ckad","source":"@site/blog/2023-03-16-ckad-notes.md","title":"Cheat Sheet for CKAD","description":"Cracking CKAD examination requires not only a solid comprehension of Kubernetes concepts but also proficiency in executing K8 commands. In this following, I have shared notes which I took during my CKAD preparation. I faced difficulty memorizing all Kubernetes object YAMLs and imperative commands, I consolidate all configurations in a single location as a comprehensive revision guide before the exam. I hope these notes can help you out too!","date":"2023-03-16T00:00:00.000Z","formattedDate":"March 16, 2023","tags":[{"label":"ckad","permalink":"/blog/tags/ckad"},{"label":"kubernetes","permalink":"/blog/tags/kubernetes"}],"readingTime":9.97,"hasTruncateMarker":true,"authors":[{"name":"Kundan Kumar","title":"Software Engineer at JP Morgan","url":"https://github.com/kundank78","imageURL":"https://avatars.githubusercontent.com/u/25195457"}],"frontMatter":{"slug":"cheat-sheet-ckad","title":"Cheat Sheet for CKAD","author":"Kundan Kumar","author_url":"https://github.com/kundank78","author_title":"Software Engineer at JP Morgan","author_image_url":"https://avatars.githubusercontent.com/u/25195457","tags":["ckad","kubernetes"]},"prevItem":{"title":"Here I am","permalink":"/blog/here-i-am"},"nextItem":{"title":"GitHub Action for Docusaurus","permalink":"/blog/docusaurus-gh-action"}},"content":"Cracking CKAD examination requires not only a solid comprehension of Kubernetes concepts but also proficiency in executing K8 commands. In this following, I have shared notes which I took during my CKAD preparation. I faced difficulty memorizing all Kubernetes object YAMLs and imperative commands, I consolidate all configurations in a single location as a comprehensive revision guide before the exam. I hope these notes can help you out too!\\n\\n\x3c!--truncate--\x3e\\n\\n### Setting Alias ..... A Must\\n```\\nalias k=kubectl\\n\\nexport do=\\"--dry-run=client -o yaml\\" \\nk create deploy nginx --image=nginx $do\\n\\nexport now=\\"--force --grace-period 0\\"\\nk delete pod pod_name $now\\n\\n```\\n\\n### K8s Cluster\\n- `k cluster-info`\\n- `k get nodes`\\n\\n### ReplicaSet\\n- `k get rs`\\n- `k scale --replicas=new_number rs replica_set_name`\\n\\n```yml\\n---\\napiVersion: apps/v1\\nkind: ReplicaSet\\nmetadata:\\n  name: replica_set_name\\n  labels:\\n    key: value\\nspec:\\n  replicas: no_of_pods\\n  selector:  #used to identify existing pods in env\\n    matchLabels:\\n      key: value\\n  template:\\n    metadata:\\n      name: pod_name\\n      labels:\\n        key: value\\n    spec:\\n      containers:\\n        - name: container_name\\n          image: image_name\\n```\\n\\n### Deployment\\n- `k create deploy deployment_name --image=image_name --replicas=no_of_pods`\\n- `k scale deploy deploy_name --replicas=no_of_pods`\\n- `k edit deploy deploy_name`                                              ----\x3e edit any field of deployment\\n- `k set image deploy deploy_name container_name=nginx:1.9.1 --record`     ----\x3e changed image to different version with record flag capturing cmd used\\n\\n- `k rollout status deploy deploy_name`\\n- `k rollout history deploy deploy_name`                                   ----\x3e show revisions of deployment\\n- `k rollout undo deploy deploy_name`                                      ----\x3e undo deployment to last revision\\n- `k rollout history deploy deploy_name --revision=number`                 ----\x3e describe deployment of revision number\\n- `k rollout undo deploy deploy_name --to-revision=number`                 ----\x3e rollback deployment to specific version\\n\\n```yml\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: deployment_name\\n  labels:\\n    key: value\\nspec:\\n  replicas: no_of_pods\\n  strategy:\\n    rollingUpdate:\\n      maxSurge: 25%\\n      maxUnavailable: 25%\\n    type: RollingUpdate\\n  selectors:\\n    matchLabels:\\n      key: value\\n  template:\\n    metadata:\\n      name: pod_name\\n      labels:\\n        key: value\\n    spec:\\n      containers:\\n        - name: container_name\\n          image: image_name\\n          ports:\\n            - containerPort: 8080\\n```\\n\\n### Namespace\\n- `k config set-context --current --namespace=namespace_name`\\n- `k port-forward svc/my-service 5000`\\n\\n```yml\\n---\\napiVersion: v1\\nkind: ResourceQuota\\nmetadata:\\n  name: compute-quota\\n  namespace: dev\\nspec:\\n  hard:\\n    pods: \\"10\\"\\n    requests:\\n      cpu: \\"4\\"\\n      memory: \\"5Gi\\"\\n    limits:\\n      cpu: \\"10\\"\\n      memory: 10Gi\\n```\\n\\n### Pods Configuration\\n\\n##### Multi container pod: share same lifecycle & network\\n- Patterns: Side Car, Ambassador, Adapter\\n\\n- POD Conditions\\n- PodScheduled -> Initialized -> ContainersReady -> Ready\\n\\n- `k logs -f pod_name container_name`               # tail logs for container_name for multi container pod\\n- `k replace -f pod.yaml --force`                   # replace existing pod with new one\\n- `k get pods -l key=value --no-headers | wc -l`    # count of pod\\n- `k get pod --show-labels`\\n- `k label po -l \\"app in(v1,v2)\\" tier=web`\\n\\n##### Taint Node\\n- `k taint node node_name key=value:taint-effect`  | Effects: NoSchedule, PreferNoSchedule, NoExecute\\n- `k taint node node_name key=value:taint-effect-` | Remove taint\\n- `k label node node_name key=value`\\n\\n```yml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod_name\\n  labels:\\n    key: value\\n  annotations:\\n    buildVersion: 1.34\\n  namespace: ns_name\\nspec:\\n  restartPolicy: Always                   # Always by default, Never & OnFailure\\n  serviceAccountName: service_acc_nm      # cannot be edited in pod but in deployment this can be edited and deployment will handle rollout for us\\n  automountServiceAccountToken: false     # doesn\'t mount default service account\\n  securityContext:                        # pod level security context, can be moved to container level\\n    runAsUser: 1000\\n  volumes:\\n    - name: pvc_volume\\n      persistentVolumeClaim:\\n        claimName: pvc_name\\n    - name: config_map_volume\\n      configMap:\\n        name: config_map_name\\n    - name: secret_volume                 # creates file for each key & value as secret\\n      secret:\\n        secretName: secret_name\\n    - name: empty_dir_vol                 # exists as long as pod on node disk, ram or network storage\\n      emptyDir:\\n        sizeLimit: 500Mi\\n    - name: host_volume\\n      hostPath:                           # mount file or directory from host node\'s filesystem\\n        type: Directory | DirectoryOrCreate\\n        path: /data\\n  affinity:\\n    nodeAffinity:\\n      requiredDuringSchedulingIgnoredDuringExecution:    # preferredDuringSchedulingIgnoredDuringExecution\\n        nodeSelectorTerms:\\n          - matchExpressions:\\n              - key: size\\n                operator: NotIn                          # In, NotIn, Exists, DoesNotExist, Gt and Lt.\\n                values:\\n                  - Small\\n  initContainers:\\n    - name: install\\n      image: image_name\\n  containers:\\n    - name: container_name\\n      image: image_name\\n      ports:\\n        - containerPort: port to expose\\n      command: [\\"sleep2.0\\"]               # overrides the entrypoint in docker\\n      args: [\\"10\\"]                        # adds as param in docker run cmd\\n      resources:                          # Scheduled on node if sum of requests of all container is less than node limit\\n        requests:\\n          cpu: \\"1\\"                        # \\"1\\" -> 1000m | 1m -> 1v cpu aws | If request not specified, it matches limit (same for memory)\\n          memory: \\"512Mi\\"\\n        limits:                           # container cannot exceeds its cpu limit    \\n          cpu: \\"2\\"                        # container can use more memory than limit but if it is continuous it will terminate | If limit not specified, \\n          memory: \\"1Gi\\"                     it can use all of node\'s memory and finally get killed | If limit range is defined for namespace it uses it as default\\n      securityContext:\\n        capabilities:\\n          add: [ \\"MAC_ADMIN\\" ]\\n      volumeMounts:\\n        - mountPath: /opt                 # this will be in sync\\n          name: volume_name\\n      env:                                # adding env variables\\n        - name: APP_COLOR\\n          value: pink\\n        - name: APP_COLOR\\n          valueFrom:\\n            configMapKeyRef:\\n              name: config_map_name\\n              key: KEY1\\n        - name: APP_COLOR\\n          valueFrom:\\n            secretKeyRef:\\n              name: secret_name\\n              key: KEY1\\n      envFrom:\\n        - configMapRef:\\n            name: config_map_name\\n        - secretRef:\\n            name: secret_name\\n      tolerations:\\n        - key: \\"app\\"\\n          operator: \\"Equal\\"\\n          value: \\"blue\\"\\n          effect: \\"NoSchedule\\"\\n      nodeSelector:                         # label node with same\\n        key: value\\n      readinessProbe:\\n        periodSeconds: 5\\n        initialDelaySeconds: 15\\n        failureThreshold: 8\\n        httpGet:\\n          path: /api/ready\\n          port: 5000\\n        tcpSocket:\\n          port: 8080\\n        exec:\\n          command:\\n            - cat\\n            - /app/is_ready\\n      livenessProbe:\\n        periodSeconds: 5\\n        initialDelaySeconds: 15\\n        failureThreshold: 8\\n        httpGet:\\n          path: /api/ready\\n          port: 5000\\n        tcpSocket:\\n          port: 8080\\n        exec:\\n          command:\\n            - cat\\n            - /app/is_ready\\n```\\n\\n### Config Map\\n\\n- `k create configmap config_map_name --from-literal=KEY1=VALUE1 --from-literal=KEY2=VALUE2`\\n- `k create configmap config_map_name --from-file=file_name.properties`\\n\\n```yml\\n---\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: config_map_name\\nspec:\\n  key1: value1\\n  key2: value2\\n```\\n\\n### Secrets\\n- `echo -n \'encode\' | base64`\\n- `echo -n \'decode\' | base64 --decode`\\n- `k create secret secret_name --from-literal=KEY1=VALUE1 --from-literal=KEY2=VALUE2`\\n- `k create secret secret_name --from-file=file_name.properties`\\n\\n```yml\\n---\\napiVersion: v1\\nkind: Secret\\nmetadata:\\n  name: secret_name\\nspec:\\n  key1: value1\\n  key2: value2\\n```\\n### Service Accounts\\n```\\nwhen service account is created a token is generated as secret | This used happen before kube 1.24, after\\n1.24 you can create token using `k create token service_account_name` this prints token with expiry time\\nToken can be passed as a bearer token while calling kube apis\\nEach namespace has its own service account\\nPod when created is mounted with volume having token created via token request api(has expiry)\\n```\\n``` \\n k create sa service_account_name\\n k create token <sa_name>\\n``` \\nManually create long-lived token for service account\\nAnnotate secret with kubernetes.io/service-account.name: <sa_name> and controller will auto-inject token inside the secret\\n\\n\\n\\n### Jobs & CronJob\\n- `k create job job_name --image=image_name -- command`\\n- `k create cronjob cron_name --image=image_name --schedule=\\"* * * * *\\"`\\n\\n```yml\\n---\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  name: job_name\\nspec:\\n  completions: 3                            ----\x3e no. of successful job completions | new pods are created until this number is reached\\n  parallelism: 3                            ----\x3e creates pods in parallel\\n  template:\\n    spec:\\n      containers:\\n        - name: container_name\\n          image: image_name\\n          command:\\n            - cmd1\\n      restartPolicy: Never\\n\\n---\\napiVersion: batch/v1beta1\\nkind: CronJob\\nmetadata:\\n  name: cron-job-name\\nspec:\\n  schedule: \\"*/1 * * * *\\"\\n  jobTemplate:\\n    spec:\\n      completions: 3\\n      parallelism: 3\\n      template:\\n        spec:\\n          containers:\\n            - name: container_name\\n              image: image_name\\n          restartPolicy: Never\\n```\\n\\n\\n### Service\\n```yml\\n---\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: service_name\\nspec:\\n  type: NodePort\\n  ports:\\n    - targetPort: 80                                   # container\'s port\\n      port: 80                                         # service port\\n      nodePort: 30008\\n  selector:\\n      key: value\\n      key1: value1\\n```\\n\\n##### Load Balancing through services is done in random fashion\\n##### ClusterIP : creates a virtual ip\\n##### LoadBalancer\\n##### NodePort (Range: 30000 - 32767): Exposes node port for every node if pods are distributed\\n\\n- `k expose pod pod_name --port=port_number --name=svc_name`        ----\x3e create cluster ip service with pod\'s label as selectors\\n- `k create svc clusterip svc_name --tcp=?:?`                       ----\x3e create cluster ip service with selector\'s as app=svc_name\\n\\n- `k expose pod pod_name --port=80 --type=NodePort`                 ----\x3e create node port service with pod\'s label as selectors\\n- `k create svc nodeport svc_name --tcp=?:? --node-port=node_port`  ----\x3e create nodeport svc with defined node port but doesn\'t use pod\'s labels as selectors\\n\\n\\n### Ingress\\n\\n```yml\\n---\\napiVersion: networking.k8s.io/v1\\nkind: Ingress\\nmetadata:\\n  name: ingress_name\\nspec:\\n  rules:\\n    - host: host_name\\n      http:\\n        paths:\\n          - path: /path1\\n            pathType: Prefix\\n            backend:\\n              service:\\n                name: svc_name\\n                port:\\n                  number: port_number\\n          - path: /path2\\n            pathType: Prefix\\n            backend:\\n              service:\\n                name: svc_name2\\n                port:\\n                  number: port_number2\\n```\\n\\n- `kubectl create ingress <ingress-name> --rule=\\"host/path=service:port\\"`\\n- `kubectl create ingress ingress-test --rule=\\"wear.my-online-store.com/wear*=wear-service:80\\"`\\n\\n### Network Policy\\n\\n```yml\\n---\\napiVersion: networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata:\\n  name: name\\n  namespace: pod_namespace\\nspec:\\n  podSelector:\\n    matchLabels:\\n      key: value\\n  policyTypes:                           # If only policyTypes is present it blocks all Ingress & Egress traffic\\n    - Ingress\\n    - Egress\\n  ingress:\\n    - from:\\n        - podSelector:\\n            matchLabels:\\n              key: other-pod\\n          namespaceSelector:             # If only namespace selector defined all pods under given ns will be able to access\\n            matchLabels:\\n              name: other_ns_name\\n        - ipBlock:\\n            cidr: 192.168.5.10/32\\n      ports:\\n        - protocol: TCP\\n          port: 3306                     # incoming traffic on port\\n  egress:\\n    to:\\n      - ipBlock:\\n          cidr: 192.168.5.10/32\\n    ports:\\n      - protocol: TCP\\n        port: 80                         # port on server ip\\n```\\n\\n### Volumes\\n\\n```yml\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: pv_name\\nspec:\\n  accessModes:\\n    - ReadWriteOnce\\n    - ReadOnlyMany\\n    - ReadWriteMany\\n  capacity:\\n    storage: 1Gi\\n  persistentVolumeReclaimPolicy: Recycle | Retain | Delete\\n  awsElasticBlockStore:\\n    volumeID: volume_id\\n    fsType: ext4\\n  hostPath:\\n    path: \\"/mnt/data\\"\\n\\n---\\napiVersion: v1\\nkind: PersistentVolumeClaim\\nmetadata:\\n  name: pvc_name\\nspec:\\n  storageClassName: manual | normal\\n  accessModes:\\n    - ReadWriteOnce\\n  resources:\\n    requests:\\n      storage: 500Mi\\n\\n```\\n\\n### Kube API Server & API Groups\\n\\n##### Edit the kube-apiserver static pod configured by kubeadm to pass in the user details.\\n##### The file is located at /etc/kubernetes/manifests/kube-apiserver.yaml\\n\\n- `k config use-context context_name`\\n- `k config view`\\n- `k config set-context --current --namespace=name`\\n\\n\\n### API Groups\\n- `k proxy`     ---\x3e exposes kube api on local\\n- `k api-resources --namespaced=true `\\n- Actions- list, get, create, update, delete, watch\\n\\n```yml\\n---\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: kube-apiserver\\n  namespace: kube-system\\nspec:\\n  containers:\\n    - command:\\n        - kube-apiserver\\n        - --authorization-mode=Node,RBAC\\n          <content-hidden>\\n        - --basic-auth-file=/tmp/users/user-details.csv\\n      image: k8s.gcr.io/kube-apiserver-amd64:v1.11.3\\n      name: kube-apiserver\\n      volumeMounts:\\n        - mountPath: /tmp/users\\n          name: usr-details\\n          readOnly: true\\n  volumes:\\n    - hostPath:\\n        path: /tmp/users\\n        type: DirectoryOrCreate\\n      name: usr-details\\n```\\n\\n### Roles\\n- `k create role role_name --verb=get,ist --resources=pods,pods/status --resource-name`\\n- `k create rolebinding role_binding_name --clusterrole=cluster_role_name --user=user_name --namespace=ns_name`\\n- `k auth can-i <cmd> -as user`\\n\\n```yml\\nkind: Role\\napiVersion: rbac.authorization.k8s.io/v1\\nmetadata:\\n  namespace: default\\n  name: pod-reader\\nrules:\\n  - apiGroups: [\\"\\"] # \\"\\" indicates the core API group\\n    resources: [\\"pods\\", \\"pods/log\\"]\\n    verbs: [\\"get\\", \\"watch\\", \\"list\\"] | # [\\"*\\"] --\x3e everything\\n\\n---\\n# This role binding allows \\"user1\\" to read pods in the \\"default\\" namespace.\\nkind: RoleBinding\\napiVersion: rbac.authorization.k8s.io/v1\\nmetadata:\\n  name: read-pods\\n  namespace: default\\nsubjects:\\n  - kind: User\\n    name: user1 # Name is case-sensitive\\n    apiGroup: rbac.authorization.k8s.io\\nroleRef:\\n  kind: Role #this must be Role or ClusterRole\\n  name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to\\n  apiGroup: rbac.authorization.k8s.io\\n```\\n\\n\\n### API Versions\\n\\n##### vXalphaY --\x3e vXbetaY --\x3e vX\\n\\n##### Alpha -> Not enabled by default\\n##### Beta -> Enabled by default\\n##### GA(Stable) -> Enabled by default\\n\\n##### Preferred version -> version k8s will use while retrieving info\\n##### Storage Version -> version objects will converted to while storing in etcd cluster\\n\\n### API Deprecations\\n1. Api elements can be removed only by incrementing API version of group\\n2. Api objects must be able to round trip between API versions in a given release without information loss\\n   with exception of whole REST resources which don\'t exist in some version\\n3. Other than the most recent API version in each track, older API version must be supported after their announced\\n   deprecation for a duration of no less than-\\n   a. GA (stable)- 12 months or 3 releases (whichever is longer)\\n   b. Beta - 9 months or 3 releases (whichever is longer)\\n   c. Alpha - 0 releases\\n   In Kubernetes versions -> X.Y.Z\\n   Where X stands for major, Y stands for minor and Z stands for patch version.\\n- `k convert -f <old_file> --output-version group/version`\\n- Add `--runtime-config={api-group}/{version}`  --\x3e enable new version\\n\\n### Helm\\n\\n```\\nhelm repo add repository-name url\\nhelm repo remove repository-name\\nhelm repo update\\nhelm list\\nhelm search hub package_name\\nhelm search repo package_name\\nhelm show chart repo/package\\nhelm show values repo/package\\n\\nhelm get manifest release_name\\nhelm install release_name chart_name\\nhelm status release_name\\nhelm upgrade release_name repo/package\\nhelm history release_name\\nhelm rollback release_name version\\nhelm uninstall release-name\\nhelm pull --untar repo/chart_name\\n```"},{"id":"docusaurus-gh-action","metadata":{"permalink":"/blog/docusaurus-gh-action","source":"@site/blog/2021-01-17-docusaurus-gh-action.md","title":"GitHub Action for Docusaurus","description":"I got tired of deploying my Docusaurus website to GitHub Pages manually, and decided to do something about it using GitHub Action.","date":"2021-01-17T00:00:00.000Z","formattedDate":"January 17, 2021","tags":[{"label":"docusaurus","permalink":"/blog/tags/docusaurus"},{"label":"github-action","permalink":"/blog/tags/github-action"},{"label":"ci","permalink":"/blog/tags/ci"}],"readingTime":1.485,"hasTruncateMarker":true,"authors":[{"name":"Kundan Kumar","title":"Software Engineer at JP Morgan","url":"https://github.com/kundank78","imageURL":"https://avatars.githubusercontent.com/u/25195457"}],"frontMatter":{"slug":"docusaurus-gh-action","title":"GitHub Action for Docusaurus","author":"Kundan Kumar","author_url":"https://github.com/kundank78","author_title":"Software Engineer at JP Morgan","author_image_url":"https://avatars.githubusercontent.com/u/25195457","tags":["docusaurus","github-action","ci"]},"prevItem":{"title":"Cheat Sheet for CKAD","permalink":"/blog/cheat-sheet-ckad"}},"content":"I got tired of deploying my Docusaurus website to GitHub Pages manually, and decided to do something about it using GitHub Action.\\n\\nInitially, I was planning to follow the [official guide](https://v2.docusaurus.io/docs/deployment#triggering-deployment-with-github-actions) on doing so. However, it was actually much more complicated than I liked. I did not really want to generate and store a SSH key on GitHub. Too much effort man.\\n\\nI decided it was better off for me to write my own script. Here it is:\\n\\n\x3c!--truncate--\x3e\\n\\n## deploy-docusaurus.yml\\n\\n:::caution\\n\\nThe script below assumes that your Docusaurus website resides at `/website` of your repo. If that is not the case for you, you will need to:\\n\\n- Change `cd website` to `cd <docu_site_root>`, or delete the entire line if your Docusaurus website is at the root of your repo `/`\\n- Change `build_dir`\'s value from `website/build` to `<docu_site_root>/build`, or `build` if your Docusaurus website is at the root of your repo `/`\\n\\n:::\\n\\n```yml\\nname: deploy-docusaurus\\n\\non:\\n  push:\\n    branches: [main]\\n  pull_request:\\n    branches: [main]\\n\\n  # Allows you to run this workflow manually from the Actions tab\\n  workflow_dispatch:\\n\\n# A workflow run is made up of one or more jobs that can run sequentially or in parallel\\njobs:\\n  publish:\\n    runs-on: ubuntu-latest\\n    steps:\\n      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it\\n      - name: Check out repo\\n        uses: actions/checkout@v2\\n      # Node is required for npm\\n      - name: Set up Node\\n        uses: actions/setup-node@v2\\n        with:\\n          node-version: \\"12\\"\\n      # Install and build Docusaurus website\\n      - name: Build Docusaurus website\\n        run: |\\n          cd website\\n          npm install \\n          npm run build\\n      - name: Deploy to GitHub Pages\\n        if: success()\\n        uses: crazy-max/ghaction-github-pages@v2\\n        with:\\n          target_branch: gh-pages\\n          build_dir: website/build\\n        env:\\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\\n```\\n\\n:::note\\n\\nGitHub will automatically add `GITHUB_TOKEN` to Secrets. You need not do so. See [this](https://docs.github.com/en/actions/reference/authentication-in-a-workflow) for more information.\\n\\n:::"}]}')}}]);